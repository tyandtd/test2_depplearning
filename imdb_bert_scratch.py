import os
import sys
import logging
import datasets

import torch.nn as nn

import pandas as pd
import numpy as np

from transformers import BertTokenizerFast, DataCollatorWithPadding
from transformers import Trainer, TrainingArguments
from transformers import BertPreTrainedModel, BertModel
from transformers.modeling_outputs import SequenceClassifierOutput

from sklearn.model_selection import train_test_split

# train = pd.read_csv("./corpus/imdb/labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)
# test = pd.read_csv("./corpus/imdb/testData.tsv", header=0, delimiter="\t", quoting=3)
train = pd.read_csv("F:\\deeplearning\\test2\\imdb_sentiment_analysis_torch\\labeledTrainData.tsv", header=0,
                    delimiter="\t", quoting=3)
test = pd.read_csv("F:\\deeplearning\\test2\\imdb_sentiment_analysis_torch\\testData.tsv", header=0,
                   delimiter="\t", quoting=3)

class BertScratch(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = BertModel(config)
        classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = nn.Dropout(classifier_dropout)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.post_init()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        outputs = self.bert(input_ids, attention_mask, token_type_ids)

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
#é—®é¢˜æ˜¯åœ¨è¿™ä¸€å¥ï¼Œè¿™ä¸ªåœ°æ–¹æ²¡æœ‰å°†ä¸åŒçš„æ¨¡å‹å¸¦å…¥ä¸€ä¸ªåˆ†ç±»ï¼Œæˆ‘ç°åœ¨åƒçš„æ˜¯ï¼Œå°†ä¸åŒçš„è¾“å…¥ï¼Œæ”¹ä¸ºä¸åŒçš„å¯¹åº”çš„å‡½æ•°ï¼Œä»…åœ¨è¾“å…¥ä¸ºè®­ç»ƒçš„æ—¶å€™è¾“å…¥
#æˆ‘åœ¨pythonä¸­å¸¸è§çš„æ˜¯ï¼Œåœ¨trainâ€”â€”loopä¸­åŒ…å«lossï¼Œè¿™ä¸ªåœ¨è¿™ä¸ªä½ç½®åŒ…å«ï¼Œç„¶åç¨å¾®æœ‰ä¸€ç‚¹è¾“å…¥é”™è¯¯ï¼Œæˆ–è€…è¯´ï¼Œè¿™ä¸ªç±»çš„æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè®­ç»ƒæ± ã€‚    
        loss_fct = nn.CrossEntropyLoss()
        if labels != None:
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        else:
            loss = None
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions
        )


if __name__ == '__main__':
    program = os.path.basename(sys.argv[0])
    logger = logging.getLogger(program)

    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
    logging.root.setLevel(level=logging.INFO)
    logger.info(r"running %s" % ''.join(sys.argv))
#æ—¥å¿—æ–‡ä»¶
    train, val = train_test_split(train, test_size=.2)

    train_dict = {'label': train["sentiment"], 'text': train['review']}
    val_dict = {'label': val["sentiment"], 'text': val['review']}
    test_dict = {"text": test['review']}

    train_dataset = datasets.Dataset.from_dict(train_dict)
    val_dataset = datasets.Dataset.from_dict(val_dict)
    test_dataset = datasets.Dataset.from_dict(test_dict)

    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')


    def preprocess_function(examples):
        return tokenizer(examples['text'], truncation=True)


    tokenized_train = train_dataset.map(preprocess_function, batched=True)
    tokenized_val = val_dataset.map(preprocess_function, batched=True)
    tokenized_test = test_dataset.map(preprocess_function, batched=True)
#åˆ°è¿™ä¸ªåœ°æ–¹å®Œæˆäº†åŸºæœ¬çš„è¯»å–æ–‡ä»¶
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
#åœ¨è¿™ä¸ªåœ°æ–¹å°±æ˜¯è‡ªå®šä¹‰çš„æ¨¡å‹BertScratchäº†
    model = BertScratch.from_pretrained('bert-base-uncased')

    metric = datasets.load_metric("accuracy")
#åœ¨ä¸­å›½åœ°æ–¹åŠ è½½äº†æ–‡ä»¶çš„ç±»ï¼Œç±»ä¼¼äºgooleçš„pickleæ–‡ä»¶

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)


    training_args = TrainingArguments(
        output_dir='./checkpoint',  # output directory
        num_train_epochs=3,  # total number of training epochs
        per_device_train_batch_size=6,  # batch size per device during training
        per_device_eval_batch_size=12,  # batch size for evaluation
        warmup_steps=500,  # number of warmup steps for learning rate scheduler
        weight_decay=0.01,  # strength of weight decay
        logging_dir='./logs',  # directory for storing logs
        logging_steps=100,
        save_strategy="no",
        evaluation_strategy="epoch"
    )

    trainer = Trainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=tokenized_train,  # training dataset
        eval_dataset=tokenized_val,  # evaluation dataset
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
#è¿™ä¸€å¥å°±æ˜¯æ™®é€šçš„è®­ç»ƒæ¨¡å‹
    trainer.train()
#ç›®å‰æ˜¯è¿™ä¸€å¥æœ‰é—®é¢˜ï¼Œtokenized_testæ–‡ä»¶æœªå…·æœ‰ä¸€ä¸ªç‰¹å®šçš„å±æ€§ï¼Œå¯¼è‡´å…¶ä¸­çš„å‡½æ•°è°ƒç”¨æ­¤å±æ€§æ—¶ï¼Œå‘ç”Ÿé—®é¢˜
    prediction_outputs = trainer.predict(tokenized_test)
    test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()
    print(test_pred)

    result_output = pd.DataFrame(data={"id": test["id"], "sentiment": test_pred})
    result_output.to_csv("F:\\deeplearning\\test2\\imdb_sentiment_analysis_torch\\result\\bert_scratch.csv", index=False, quoting=3)
    logging.info('result saved!')
   #ç›®å‰æ˜¯è¿™ä¸€å¥æœ‰é—®é¢˜ï¼Œtokenized_testæ–‡ä»¶æœªå…·æœ‰ä¸€ä¸ªç‰¹å®šçš„å±æ€§ï¼Œå¯¼è‡´å…¶ä¸­çš„å‡½æ•°è°ƒç”¨æ­¤å±æ€§æ—¶ï¼Œå‘ç”Ÿé—®é¢˜ 